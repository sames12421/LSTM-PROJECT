{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUklehkivjeuHxqIa0VbAa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sames12421/LSTM-PROJECT/blob/main/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyWa2sUs3QrW"
      },
      "outputs": [],
      "source": [
        "#self state\n",
        "#cell state"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "76c924tpDyOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=['i love deep learning','deep learning is amazing ', 'adi loves me more']"
      ],
      "metadata": {
        "id": "YlYu8jXxD-ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizer makes into small unit\n",
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts(text)\n",
        "sequences=tokenizer.texts_to_sequences(text)"
      ],
      "metadata": {
        "id": "GirqvDggEPOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pad Sequences\n",
        "max_length = max([len(seq) for seq in sequences])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')"
      ],
      "metadata": {
        "id": "irWT-EnOEoG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    LSTM(50, activation='relu', return_sequences=True, input_shape=(max_length, 1)),\n",
        "    Dense(len(tokenizer.word_index) + 1, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "N9uZGnfYIgXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = to_categorical(padded_sequences, num_classes=len(tokenizer.word_index) + 1)"
      ],
      "metadata": {
        "id": "3SYM4zF7O6hV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(padded_sequences, y_train, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mT4gxRDOurR",
        "outputId": "71fac5c0-2644-4dbd-f5d8-ed9eb80c6fa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 0.1667 - loss: 2.4165\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.1667 - loss: 2.4082\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.1667 - loss: 2.4002\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.1667 - loss: 2.3924\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.1667 - loss: 2.3848\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.1667 - loss: 2.3773\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.1667 - loss: 2.3700\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.1667 - loss: 2.3629\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 2.3559\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.1667 - loss: 2.3489\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a5e97fbbcd0>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# prompt: summary of above code\n",
        "\n",
        "This code creates and trains a simple LSTM (Long Short-Term Memory) recurrent neural network using TensorFlow/Keras to process text data.  Let's break down the key steps:\n",
        "\n",
        "1. **Data Preparation:**\n",
        "   - A small list of text strings (`text`) is defined as the training data.\n",
        "   - `Tokenizer` is used to convert words into numerical sequences, creating a vocabulary.\n",
        "   - `texts_to_sequences` transforms the text into integer sequences based on the vocabulary.\n",
        "   - `pad_sequences` adds padding to make all sequences the same length, which is necessary for LSTM input.\n",
        "\n",
        "2. **Model Building:**\n",
        "   - A sequential model is created.\n",
        "   - An LSTM layer with 50 units and ReLU activation is added.  `return_sequences=True` indicates that the LSTM layer should output the hidden state for each timestep in the sequence.  The input shape is specified as (max_length, 1), where `max_length` is the length of the padded sequences.\n",
        "   - A dense (fully connected) output layer is added with a softmax activation. The number of output units is equal to the vocabulary size plus one, allowing the model to predict the probability of each word in the vocabulary.\n",
        "\n",
        "3. **Model Compilation:**\n",
        "   - The model is compiled using the Adam optimizer, categorical cross-entropy loss (suitable for multi-class classification), and accuracy as the evaluation metric.\n",
        "\n",
        "4. **Training:**\n",
        "   - The target variable `y_train` is created using `to_categorical`, representing the padded sequences as one-hot encoded vectors.\n",
        "   - The model is trained using the padded sequences as input and the one-hot encoded sequences as the target, for 10 epochs.\n",
        "\n",
        "\n",
        "In essence, the code trains an LSTM model to predict the next word in a sequence, based on the provided short text examples.  It's a basic example and would likely need more data and a more sophisticated architecture for real-world applications.\n"
      ],
      "metadata": {
        "id": "QqTs_h9gP7iy"
      }
    }
  ]
}